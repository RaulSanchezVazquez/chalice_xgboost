{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless XGBoost\n",
    "\n",
    "There are many ways that data scientists can contribute to the projects or companies they work with. Arguably, one of the biggest contributions that a data scientist can make is to deploy a model that makes inference in real-time on an online setting. \n",
    "\n",
    "When it comes to deploying models, such as XGBoost models, there are various tools that allow you to achieve the same and ultimate goal: online inference. This tutorial will illustrate my favorite choice in the context of XGBoost, that consists of a small set of technologies easy to understand, very reliable, secure, scalable, and affordable.\n",
    "\n",
    "The reasons for me that made of this approach my favorite is mainly due to the following advantages:\n",
    "\n",
    "- There is no need to install any package in the production environment\n",
    "- It is very affordable for all project sizes.\n",
    "- It is powered by cloud solutions that made it very scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Deploy a XGBoost Binary Classifier.\n",
    "\n",
    "## Step 1: Fit a XGBoost Binary Classifier.\n",
    "\n",
    "In this section we'll fit a binary XGBoost classifier to the Breast Cancer dataset.\n",
    "\n",
    "In the dataset, we will artificially insert a few `NaN`. This will allow us to show that the pure python code correctly handles missing values. Also, we will use `pandas.DataFrame` so that the split nodes in the XGBoost trees contain the feature names, which will make our tree structure a bit more human readable.\n",
    "And last, we will use early stopping in order to show how to correctly fetch scores from the top-N trees.\n",
    "\n",
    "The resulting model will be saved in JSON format insead to the classic pickle format. By saving the model as a JSON instead of Pickle format will allow us to skip the XGBoost instalation in our production setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import json\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X = pd.DataFrame(\n",
    "    dataset.data,\n",
    "    columns=dataset.feature_names)\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values count in original dataset: 0\n",
      "NaN values count in artificial dataset: 222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lsanchez/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "nan_count_original = X.isnull().sum().sum()\n",
    "for col in X.columns:\n",
    "    X.loc[\n",
    "        X[col] <= X[col].quantile(.01),\n",
    "        col] = pd.np.nan\n",
    "nan_count_artificial = X.isnull().sum().sum()\n",
    "\n",
    "print('NaN values count in original dataset: %s' % nan_count_original)\n",
    "print('NaN values count in artificial dataset: %s' % nan_count_artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.070175\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.070175\n",
      "[2]\tvalidation_0-error:0.070175\n",
      "[3]\tvalidation_0-error:0.070175\n",
      "[4]\tvalidation_0-error:0.070175\n",
      "[5]\tvalidation_0-error:0.070175\n",
      "[6]\tvalidation_0-error:0.070175\n",
      "[7]\tvalidation_0-error:0.070175\n",
      "[8]\tvalidation_0-error:0.070175\n",
      "[9]\tvalidation_0-error:0.070175\n",
      "[10]\tvalidation_0-error:0.070175\n",
      "Stopping. Best iteration:\n",
      "[0]\tvalidation_0-error:0.070175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.001, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=.001)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    early_stopping_rounds=10,\n",
    "    eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees: 11\n",
      "Number of top-n best trees: 1\n"
     ]
    }
   ],
   "source": [
    "MODEL_FOLDER_PATH = os.path.expanduser(\n",
    "    '~/chalice_xgboost/chalicelib/models/')\n",
    "\n",
    "MODEL_FILE_PATH = os.path.join(\n",
    "    MODEL_FOLDER_PATH, 'xgb.json')\n",
    "\n",
    "# Make folder path\n",
    "os.makedirs(\n",
    "    MODEL_FOLDER_PATH,\n",
    "    exist_ok=True)\n",
    "\n",
    "# Save dump of trees as .json\n",
    "model._Booster.dump_model(\n",
    "    MODEL_FILE_PATH,\n",
    "    dump_format='json')\n",
    "\n",
    "# Open file just keep the best n-trees.\n",
    "with open(MODEL_FILE_PATH, 'r') as f:\n",
    "    model_json = json.loads(f.read())\n",
    "print('Number of trees: %s' % len(model_json))\n",
    "\n",
    "model_json = model_json[:model.best_ntree_limit]\n",
    "print('Number of top-n best trees: %s' % len(model_json))\n",
    "\n",
    "with open(MODEL_FILE_PATH, 'w') as f:\n",
    "    f.write(json.dumps(model_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we show how the JSON dump of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nodeid': 0,\n",
       "  'depth': 0,\n",
       "  'split': 'mean concave points',\n",
       "  'split_condition': 0.0512799993,\n",
       "  'yes': 1,\n",
       "  'no': 2,\n",
       "  'missing': 1,\n",
       "  'children': [{'nodeid': 1,\n",
       "    'depth': 1,\n",
       "    'split': 'worst radius',\n",
       "    'split_condition': 16.8299999,\n",
       "    'yes': 3,\n",
       "    'no': 4,\n",
       "    'missing': 3,\n",
       "    'children': [{'nodeid': 3,\n",
       "      'depth': 2,\n",
       "      'split': 'radius error',\n",
       "      'split_condition': 0.572100043,\n",
       "      'yes': 7,\n",
       "      'no': 8,\n",
       "      'missing': 7,\n",
       "      'children': [{'nodeid': 7, 'leaf': 0.00191752589},\n",
       "       {'nodeid': 8, 'leaf': 0}]},\n",
       "     {'nodeid': 4,\n",
       "      'depth': 2,\n",
       "      'split': 'mean texture',\n",
       "      'split_condition': 18.6800003,\n",
       "      'yes': 9,\n",
       "      'no': 10,\n",
       "      'missing': 10,\n",
       "      'children': [{'nodeid': 9, 'leaf': 0.00100000005},\n",
       "       {'nodeid': 10, 'leaf': -0.00125000009}]}]},\n",
       "   {'nodeid': 2,\n",
       "    'depth': 1,\n",
       "    'split': 'worst concave points',\n",
       "    'split_condition': 0.14655,\n",
       "    'yes': 5,\n",
       "    'no': 6,\n",
       "    'missing': 6,\n",
       "    'children': [{'nodeid': 5,\n",
       "      'depth': 2,\n",
       "      'split': 'worst perimeter',\n",
       "      'split_condition': 115.25,\n",
       "      'yes': 11,\n",
       "      'no': 12,\n",
       "      'missing': 12,\n",
       "      'children': [{'nodeid': 11, 'leaf': 0.00106666679},\n",
       "       {'nodeid': 12, 'leaf': -0.00155555562}]},\n",
       "     {'nodeid': 6,\n",
       "      'depth': 2,\n",
       "      'split': 'concavity error',\n",
       "      'split_condition': 0.112849995,\n",
       "      'yes': 13,\n",
       "      'no': 14,\n",
       "      'missing': 14,\n",
       "      'children': [{'nodeid': 13, 'leaf': -0.00192546588},\n",
       "       {'nodeid': 14, 'leaf': 0}]}]}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2**: Get model scores from the model json dump.\n",
    "\n",
    "The following two methods will allow us to fetch model scores.\n",
    "\n",
    "The first method, namely `get_tree_leaf()` will allow us to fetch a single tree leaf score value.\n",
    "And the second method: `binary_predict_proba()` will allow us to iteratively fetch model scores, sum them up and get the final probability score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_tree_leaf(node, x):\n",
    "    \"\"\"Get tree leaf score.\n",
    "    \n",
    "    Each node contains childres that are composed of aditiona nodes.\n",
    "    Final nodes with no children are the leaves.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    node: dict.\n",
    "        Node XGB dictionary.\n",
    "    x: dict.\n",
    "        Dictionary containing feature names and feature values.\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    score: float.\n",
    "        Leaf score.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'leaf' in node:\n",
    "        # If the key leaf is found, the stop recurrency.\n",
    "        score = node['leaf']\n",
    "        return score\n",
    "    else:\n",
    "        # Get current split feature value\n",
    "        x_f_val = x[node['split']]\n",
    "\n",
    "        # Get next node.\n",
    "        if str(x_f_val) == 'nan':\n",
    "            # if split feature value is nan.\n",
    "            next_node_id = node['missing']\n",
    "        elif x_f_val < node['split_condition']:\n",
    "            # Split condition is true.\n",
    "            next_node_id = node['yes']\n",
    "        else:\n",
    "            # Split condition is false.\n",
    "            next_node_id = node['no']\n",
    "\n",
    "        # Dig down to the next node.\n",
    "        for children in node['children']:\n",
    "            if children['nodeid'] == next_node_id:\n",
    "                return get_tree_leaf(children, x)\n",
    "\n",
    "\n",
    "def binary_predict_proba(x, model_json):\n",
    "    \"\"\"Get score of a binary xgboost classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: dict.\n",
    "        Dictionary containing feature names and feature values.\n",
    "    \n",
    "    model_json: dict.\n",
    "        Dump of xgboost trees as json.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_score: list\n",
    "        Scores of the negative and positve class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get tree leafs.\n",
    "    tree_leaf_scores = []\n",
    "    for tree in model_json:\n",
    "        leaf_score = get_tree_leaf(\n",
    "            node=tree,\n",
    "            x=x)\n",
    "        tree_leaf_scores.append(leaf_score)\n",
    "\n",
    "    # Get logits.\n",
    "    logit = sum(tree_leaf_scores)\n",
    "    \n",
    "    # Compute logistic function\n",
    "    pos_class_probability = 1 / (1 + math.exp(-logit))\n",
    "\n",
    "    # Get negative and positive class probabilities.\n",
    "    y_score = [1 - pos_class_probability, pos_class_probability]\n",
    "    \n",
    "    return y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually get model scores from the JSON dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.500479\n",
       "1    0.499519\n",
       "2    0.499519\n",
       "3    0.500479\n",
       "4    0.500479\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Model object\n",
    "with open(MODEL_FILE_PATH, 'r') as f:\n",
    "    model_json = json.loads(f.read())\n",
    "\n",
    "y_scores_json = pd.Series([\n",
    "    binary_predict_proba(x.to_dict(), model_json)[1]\n",
    "    for _, x in X_test.iterrows()])\n",
    "\n",
    "y_scores_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model scores via `xgboost.XGBClassifier` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.500479\n",
       "1    0.499519\n",
       "2    0.499519\n",
       "3    0.500479\n",
       "4    0.500479\n",
       "dtype: float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scores_model = pd.Series(model.predict_proba(\n",
    "    X_test\n",
    ")[:, 1])\n",
    "\n",
    "y_scores_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we have show how to create a XGBoost model, saved it as a JSON set of trees and how to get model probabilities scores using pure python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Use Chalice to deploy your model.\n",
    "\n",
    "In this section, we will use Chalice in order to deploy our serverless infrastructure in AWS. You will first need to set up your AWS credentials as shown in the following [link](https://github.com/aws/chalice). Once your AWS account and your credentials are all in place, we will only need to:\n",
    "\n",
    "- Create a new python environment and install the `chalice` package.\n",
    "- Clone the minimalist chalice project that contains our XGboost model dump.\n",
    "- Local test and deploy to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new python environment and install the `chalice` package.\n",
    "\n",
    "First, create the python environment. You can use your favorite environment management tool, in this example I'll use conda:\n",
    "```\n",
    "$ conda create --name chalice_xgboost python=3.7.3\n",
    "$ conda activate chalice_xgboost\n",
    "(chalice_xgboost) $\n",
    "```\n",
    "\n",
    "Then install chalice package:\n",
    "```\n",
    "(chalice_xgboost) $ pip install chalice\n",
    "```\n",
    "\n",
    "With all set-up, we'll go straight and clone a GitHub project in which we already have all set-up for you in order to be able to best show a minimalistic example of how to get model scores.\n",
    "\n",
    "\n",
    "## Clone the minimalist chalice project that contains our XGboost model dump.\n",
    "``` \n",
    "(chalice_xgboost) $ git clone git@github.com:RaulSanchezVazquez/chalice_xgboost.git\n",
    "```\n",
    "\n",
    "## Local test and deploy to production.\n",
    "```\n",
    "(chalice_xgboost) LuisSanchez-MBP:chalice_xgb lsanchez$ chalice local\n",
    "Serving on http://127.0.0.1:8000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean radius': 13.9,\n",
       " 'mean texture': 16.62,\n",
       " 'mean perimeter': 88.97,\n",
       " 'mean area': 599.4,\n",
       " 'mean smoothness': nan,\n",
       " 'mean compactness': 0.05319,\n",
       " 'mean concavity': 0.02224,\n",
       " 'mean concave points': 0.01339,\n",
       " 'mean symmetry': 0.1813,\n",
       " 'mean fractal dimension': 0.05536,\n",
       " 'radius error': 0.1555,\n",
       " 'texture error': 0.5762,\n",
       " 'perimeter error': 1.392,\n",
       " 'area error': 14.03,\n",
       " 'smoothness error': 0.003308,\n",
       " 'compactness error': 0.01315,\n",
       " 'concavity error': 0.009904,\n",
       " 'concave points error': 0.004832,\n",
       " 'symmetry error': 0.01316,\n",
       " 'fractal dimension error': 0.002095,\n",
       " 'worst radius': 15.14,\n",
       " 'worst texture': 21.8,\n",
       " 'worst perimeter': 101.2,\n",
       " 'worst area': 718.9,\n",
       " 'worst smoothness': 0.09384,\n",
       " 'worst compactness': 0.2006,\n",
       " 'worst concavity': 0.1384,\n",
       " 'worst concave points': 0.06222,\n",
       " 'worst symmetry': 0.2679,\n",
       " 'worst fractal dimension': 0.07698}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_test.iloc[9].to_dict()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'y_score': [0.4995206186743867, 0.5004793813256133]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "ENDPOINT = 'http://127.0.0.1:8000'\n",
    "\n",
    "body = {\"x\": x}\n",
    "\n",
    "response = http.request(\n",
    "     'POST',\n",
    "     ENDPOINT + '/predict_proba',\n",
    "     body=json.dumps(body).encode('utf-8'),\n",
    "     headers={'Content-Type': 'application/json'})\n",
    "\n",
    "response = json.loads(\n",
    "    response.data.decode('utf-8'))\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
