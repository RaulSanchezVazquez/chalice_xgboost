{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless XGBoost\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "NOTE: 2020/10/10 Work in progress...\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "There are many ways that data scientists can contribute to the projects or companies they work with. Arguably, one of the biggest contributions that a data scientist can make is to deploy a model that makes inference in real-time on an online setting. \n",
    "\n",
    "When it comes to deploying models, such as XGBoost models, there are various tools that allow you to achieve the same and ultimate goal: online inference. This tutorial will illustrate my favorite choice in the context of XGBoost, that consists of a small set of technologies easy to understand, very reliable, secure, scalable, and affordable.\n",
    "\n",
    "The reasons for me that made of this approach my favorite is mainly due to the following advantages:\n",
    "\n",
    "- There is no need to install any package in the production environment.\n",
    "- It is very affordable for all project sizes.\n",
    "- It is powered by cloud computing solutions that made it very scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a XGBoost binary classifier and get scores with pure python code.\n",
    "\n",
    "## Step 1: Fit a XGBoost Binary Classifier.\n",
    "\n",
    "In this section we'll fit a binary XGBoost classifier to the Breast Cancer dataset.\n",
    "\n",
    "In the dataset, we will artificially insert a few `NaN` values. This will allow us to demonstrate the correct handlying of missing values by our pure python code. Also, our data will use `pandas.DataFrame` so that the split nodes in the XGBoost trees contain the feature names, which will make our tree structure a bit more human readable.\n",
    "And last, we will use early stopping in order to show how to correctly fetch scores from the top-N trees of the ensemble.\n",
    "\n",
    "The resulting model will be saved in JSON format insead to the classic pickle format. By saving the model as a JSON instead of using Pickle format will allow us to skip the XGBoost instalation in our production setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read the data as a `pandas.DataFrame` and artificially insert a few `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import json\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X = pd.DataFrame(\n",
    "    dataset.data,\n",
    "    columns=[x.replace(' ', '_') for x in dataset.feature_names])\n",
    "y = dataset.target\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values count in original dataset: 0\n",
      "NaN values count in artificial dataset: 5133\n"
     ]
    }
   ],
   "source": [
    "nan_count_original = X.isnull().sum().sum()\n",
    "for col in X.columns:\n",
    "    X.loc[\n",
    "        X[col] <= X[col].quantile(.3),\n",
    "        col] = np.nan\n",
    "nan_count_artificial = X.isnull().sum().sum()\n",
    "\n",
    "print('NaN values count in original dataset: %s' % nan_count_original)\n",
    "print('NaN values count in artificial dataset: %s' % nan_count_artificial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train a xgboost model using early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.052632\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.052632\n",
      "[2]\tvalidation_0-error:0.052632\n",
      "[3]\tvalidation_0-error:0.052632\n",
      "[4]\tvalidation_0-error:0.040936\n",
      "[5]\tvalidation_0-error:0.052632\n",
      "[6]\tvalidation_0-error:0.040936\n",
      "[7]\tvalidation_0-error:0.052632\n",
      "[8]\tvalidation_0-error:0.046784\n",
      "[9]\tvalidation_0-error:0.064327\n",
      "[10]\tvalidation_0-error:0.052632\n",
      "[11]\tvalidation_0-error:0.064327\n",
      "[12]\tvalidation_0-error:0.05848\n",
      "[13]\tvalidation_0-error:0.064327\n",
      "[14]\tvalidation_0-error:0.064327\n",
      "Stopping. Best iteration:\n",
      "[4]\tvalidation_0-error:0.040936\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=500, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "model = xgb.XGBClassifier(n_estimators=500)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    early_stopping_rounds=10,\n",
    "    eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, save the model in JSON format. \n",
    "In our case we have cloned this repository in the following path: `'~/chalice_xgboost/'`, and therefore, the path where we'll like to save the model dump is in the `chalicelib/models/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER_PATH = os.path.expanduser(\n",
    "    '~/chalice_xgboost/chalicelib/models/')\n",
    "\n",
    "MODEL_FILE_PATH = os.path.join(\n",
    "    MODEL_FOLDER_PATH, 'xgb.json')\n",
    "\n",
    "# Make folder path if it does not exists.\n",
    "os.makedirs(\n",
    "    MODEL_FOLDER_PATH,\n",
    "    exist_ok=True)\n",
    "\n",
    "# Save dump of trees as .json\n",
    "model._Booster.dump_model(\n",
    "    MODEL_FILE_PATH,\n",
    "    dump_format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we had saved the entire ensemble of trees, however, by fetching scores manually we will only need the top best n-trees chose by the early-stopping feature. As the entire ensemble can be found in the json dump, we'll remove the unnecesary trees by loading the model ensemble and re-writing the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees: 15\n",
      "Number of top-n best trees: 5\n"
     ]
    }
   ],
   "source": [
    "# Load the model dump.\n",
    "with open(MODEL_FILE_PATH, 'r') as f:\n",
    "    model_json = json.loads(f.read())\n",
    "print('Number of trees: %s' % len(model_json))\n",
    "\n",
    "# Subset the ensemple to the best n-trees.\n",
    "model_json = model_json[:model.best_ntree_limit]\n",
    "print('Number of top-n best trees: %s' % len(model_json))\n",
    "\n",
    "# Rewrite the dump with only the best n-trees.\n",
    "with open(MODEL_FILE_PATH, 'w') as f:\n",
    "    f.write(json.dumps(model_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, bellow we show how the JSON dump of 2 trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nodeid': 0,\n",
       "  'depth': 0,\n",
       "  'split': 'worst_concave_points',\n",
       "  'split_condition': 0.142349988,\n",
       "  'yes': 1,\n",
       "  'no': 2,\n",
       "  'missing': 1,\n",
       "  'children': [{'nodeid': 1,\n",
       "    'depth': 1,\n",
       "    'split': 'worst_radius',\n",
       "    'split_condition': 17.6149998,\n",
       "    'yes': 3,\n",
       "    'no': 4,\n",
       "    'missing': 3,\n",
       "    'children': [{'nodeid': 3,\n",
       "      'depth': 2,\n",
       "      'split': 'area_error',\n",
       "      'split_condition': 35.2600021,\n",
       "      'yes': 7,\n",
       "      'no': 8,\n",
       "      'missing': 7,\n",
       "      'children': [{'nodeid': 7, 'leaf': 0.189830512},\n",
       "       {'nodeid': 8, 'leaf': 0.0545454584}]},\n",
       "     {'nodeid': 4,\n",
       "      'depth': 2,\n",
       "      'split': 'mean_texture',\n",
       "      'split_condition': 19.1650009,\n",
       "      'yes': 9,\n",
       "      'no': 10,\n",
       "      'missing': 9,\n",
       "      'children': [{'nodeid': 9, 'leaf': 0},\n",
       "       {'nodeid': 10, 'leaf': -0.138461545}]}]},\n",
       "   {'nodeid': 2,\n",
       "    'depth': 1,\n",
       "    'split': 'worst_perimeter',\n",
       "    'split_condition': 97.4900055,\n",
       "    'yes': 5,\n",
       "    'no': 6,\n",
       "    'missing': 5,\n",
       "    'children': [{'nodeid': 5, 'leaf': 0.0181818195},\n",
       "     {'nodeid': 6,\n",
       "      'depth': 2,\n",
       "      'split': 'radius_error',\n",
       "      'split_condition': 5.74600077,\n",
       "      'yes': 11,\n",
       "      'no': 12,\n",
       "      'missing': 12,\n",
       "      'children': [{'nodeid': 11, 'leaf': -0.190400004},\n",
       "       {'nodeid': 12, 'leaf': -0.0545454584}]}]}]},\n",
       " {'nodeid': 0,\n",
       "  'depth': 0,\n",
       "  'split': 'worst_concave_points',\n",
       "  'split_condition': 0.142349988,\n",
       "  'yes': 1,\n",
       "  'no': 2,\n",
       "  'missing': 1,\n",
       "  'children': [{'nodeid': 1,\n",
       "    'depth': 1,\n",
       "    'split': 'worst_radius',\n",
       "    'split_condition': 17.6149998,\n",
       "    'yes': 3,\n",
       "    'no': 4,\n",
       "    'missing': 3,\n",
       "    'children': [{'nodeid': 3,\n",
       "      'depth': 2,\n",
       "      'split': 'area_error',\n",
       "      'split_condition': 35.2600021,\n",
       "      'yes': 7,\n",
       "      'no': 8,\n",
       "      'missing': 7,\n",
       "      'children': [{'nodeid': 7, 'leaf': 0.172745794},\n",
       "       {'nodeid': 8, 'leaf': 0.0501142256}]},\n",
       "     {'nodeid': 4,\n",
       "      'depth': 2,\n",
       "      'split': 'mean_texture',\n",
       "      'split_condition': 19.1650009,\n",
       "      'yes': 9,\n",
       "      'no': 10,\n",
       "      'missing': 9,\n",
       "      'children': [{'nodeid': 9, 'leaf': 0},\n",
       "       {'nodeid': 10, 'leaf': -0.129318759}]}]},\n",
       "   {'nodeid': 2,\n",
       "    'depth': 1,\n",
       "    'split': 'worst_perimeter',\n",
       "    'split_condition': 97.4900055,\n",
       "    'yes': 5,\n",
       "    'no': 6,\n",
       "    'missing': 5,\n",
       "    'children': [{'nodeid': 5, 'leaf': 0.0170257203},\n",
       "     {'nodeid': 6,\n",
       "      'depth': 2,\n",
       "      'split': 'area_error',\n",
       "      'split_condition': 21.9449997,\n",
       "      'yes': 11,\n",
       "      'no': 12,\n",
       "      'missing': 11,\n",
       "      'children': [{'nodeid': 11, 'leaf': -0.0499064028},\n",
       "       {'nodeid': 12, 'leaf': -0.173635662}]}]}]},\n",
       " {'nodeid': 0,\n",
       "  'depth': 0,\n",
       "  'split': 'mean_concave_points',\n",
       "  'split_condition': 0.0489199981,\n",
       "  'yes': 1,\n",
       "  'no': 2,\n",
       "  'missing': 1,\n",
       "  'children': [{'nodeid': 1,\n",
       "    'depth': 1,\n",
       "    'split': 'area_error',\n",
       "    'split_condition': 42.1900024,\n",
       "    'yes': 3,\n",
       "    'no': 4,\n",
       "    'missing': 3,\n",
       "    'children': [{'nodeid': 3,\n",
       "      'depth': 2,\n",
       "      'split': 'worst_radius',\n",
       "      'split_condition': 17.2950001,\n",
       "      'yes': 7,\n",
       "      'no': 8,\n",
       "      'missing': 7,\n",
       "      'children': [{'nodeid': 7, 'leaf': 0.162387654},\n",
       "       {'nodeid': 8, 'leaf': 0.0214837175}]},\n",
       "     {'nodeid': 4, 'leaf': -0.0442601293}]},\n",
       "   {'nodeid': 2,\n",
       "    'depth': 1,\n",
       "    'split': 'worst_perimeter',\n",
       "    'split_condition': 104.100006,\n",
       "    'yes': 5,\n",
       "    'no': 6,\n",
       "    'missing': 5,\n",
       "    'children': [{'nodeid': 5,\n",
       "      'depth': 2,\n",
       "      'split': 'worst_concave_points',\n",
       "      'split_condition': 0.172450006,\n",
       "      'yes': 9,\n",
       "      'no': 10,\n",
       "      'missing': 10,\n",
       "      'children': [{'nodeid': 9, 'leaf': 0.122566067},\n",
       "       {'nodeid': 10, 'leaf': -0.107704416}]},\n",
       "     {'nodeid': 6,\n",
       "      'depth': 2,\n",
       "      'split': 'worst_concavity',\n",
       "      'split_condition': 0.222950011,\n",
       "      'yes': 11,\n",
       "      'no': 12,\n",
       "      'missing': 12,\n",
       "      'children': [{'nodeid': 11, 'leaf': 0.0192245375},\n",
       "       {'nodeid': 12, 'leaf': -0.161969319}]}]}]},\n",
       " {'nodeid': 0,\n",
       "  'depth': 0,\n",
       "  'split': 'mean_concave_points',\n",
       "  'split_condition': 0.0489199981,\n",
       "  'yes': 1,\n",
       "  'no': 2,\n",
       "  'missing': 1,\n",
       "  'children': [{'nodeid': 1,\n",
       "    'depth': 1,\n",
       "    'split': 'area_error',\n",
       "    'split_condition': 42.1900024,\n",
       "    'yes': 3,\n",
       "    'no': 4,\n",
       "    'missing': 3,\n",
       "    'children': [{'nodeid': 3,\n",
       "      'depth': 2,\n",
       "      'split': 'worst_perimeter',\n",
       "      'split_condition': 113.75,\n",
       "      'yes': 7,\n",
       "      'no': 8,\n",
       "      'missing': 7,\n",
       "      'children': [{'nodeid': 7, 'leaf': 0.151806206},\n",
       "       {'nodeid': 8, 'leaf': 0.01741031}]},\n",
       "     {'nodeid': 4, 'leaf': -0.0412366688}]},\n",
       "   {'nodeid': 2,\n",
       "    'depth': 1,\n",
       "    'split': 'worst_perimeter',\n",
       "    'split_condition': 104.100006,\n",
       "    'yes': 5,\n",
       "    'no': 6,\n",
       "    'missing': 5,\n",
       "    'children': [{'nodeid': 5,\n",
       "      'depth': 2,\n",
       "      'split': 'worst_concave_points',\n",
       "      'split_condition': 0.172450006,\n",
       "      'yes': 9,\n",
       "      'no': 10,\n",
       "      'missing': 10,\n",
       "      'children': [{'nodeid': 9, 'leaf': 0.11439874},\n",
       "       {'nodeid': 10, 'leaf': -0.102243774}]},\n",
       "     {'nodeid': 6,\n",
       "      'depth': 2,\n",
       "      'split': 'mean_texture',\n",
       "      'split_condition': 67.1200027,\n",
       "      'yes': 11,\n",
       "      'no': 12,\n",
       "      'missing': 12,\n",
       "      'children': [{'nodeid': 11, 'leaf': -0.156740174},\n",
       "       {'nodeid': 12, 'leaf': -0.0323793478}]}]}]},\n",
       " {'nodeid': 0,\n",
       "  'depth': 0,\n",
       "  'split': 'worst_concave_points',\n",
       "  'split_condition': 0.142349988,\n",
       "  'yes': 1,\n",
       "  'no': 2,\n",
       "  'missing': 1,\n",
       "  'children': [{'nodeid': 1,\n",
       "    'depth': 1,\n",
       "    'split': 'worst_perimeter',\n",
       "    'split_condition': 107.599998,\n",
       "    'yes': 3,\n",
       "    'no': 4,\n",
       "    'missing': 3,\n",
       "    'children': [{'nodeid': 3,\n",
       "      'depth': 2,\n",
       "      'split': 'area_error',\n",
       "      'split_condition': 46.7900009,\n",
       "      'yes': 7,\n",
       "      'no': 8,\n",
       "      'missing': 7,\n",
       "      'children': [{'nodeid': 7, 'leaf': 0.143411368},\n",
       "       {'nodeid': 8, 'leaf': 0.0141552528}]},\n",
       "     {'nodeid': 4,\n",
       "      'depth': 2,\n",
       "      'split': 'mean_texture',\n",
       "      'split_condition': 24.9850006,\n",
       "      'yes': 9,\n",
       "      'no': 10,\n",
       "      'missing': 10,\n",
       "      'children': [{'nodeid': 9, 'leaf': -0.0954347849},\n",
       "       {'nodeid': 10, 'leaf': 0.0930837914}]}]},\n",
       "   {'nodeid': 2,\n",
       "    'depth': 1,\n",
       "    'split': 'area_error',\n",
       "    'split_condition': 21.9449997,\n",
       "    'yes': 5,\n",
       "    'no': 6,\n",
       "    'missing': 5,\n",
       "    'children': [{'nodeid': 5,\n",
       "      'depth': 2,\n",
       "      'split': 'worst_texture',\n",
       "      'split_condition': 28.5450001,\n",
       "      'yes': 11,\n",
       "      'no': 12,\n",
       "      'missing': 11,\n",
       "      'children': [{'nodeid': 11, 'leaf': 0.0831701383},\n",
       "       {'nodeid': 12, 'leaf': -0.105434693}]},\n",
       "     {'nodeid': 6,\n",
       "      'depth': 2,\n",
       "      'split': 'concavity_error',\n",
       "      'split_condition': 0.113665,\n",
       "      'yes': 13,\n",
       "      'no': 14,\n",
       "      'missing': 14,\n",
       "      'children': [{'nodeid': 13, 'leaf': -0.146517023},\n",
       "       {'nodeid': 14, 'leaf': -0.00667759869}]}]}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure above is pretty simple. It consist of a `list` of python `dict`s, where each item of the list corresponds to a single tree. Each tree is composed of and initial node, and each node may have other nested nodes, that can be found under the `children` dictionary key. The keys: `split` and `split_condition`, are the feature-name and feature-value of the split condition respectively, the split value and condition is the node split you tipically find on trees diagrams of most tree-based methods. A final node, that has no further childre, correspond to a leaf node, that has a `leaf` value, which is some sort of a fraction of the models prediction score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell bellow, we illustrate the first tree by its tree diagram, where you can corroborate the correctness of the structure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /Users/lsanchez/anaconda3/envs/py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In /Users/lsanchez/anaconda3/envs/py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In /Users/lsanchez/anaconda3/envs/py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The pgf.debug rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In /Users/lsanchez/anaconda3/envs/py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In /Users/lsanchez/anaconda3/envs/py37/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(18, 10))\n",
    "ax = plot_tree(\n",
    "    model, \n",
    "    num_trees=1, \n",
    "    ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2**: Get model scores from the model json dump.\n",
    "\n",
    "In this section we describe two methods that will allow us to fetch model scores with pure python code.\n",
    "\n",
    "The first method, namely `get_tree_leaf()` allow us to fetch a single tree leaf score value.\n",
    "And the second method: `binary_predict_proba()` allow us to iteratively fetch model scores, sum them up and get the final probability score.\n",
    "\n",
    "Both methods are well documented, feel free to take your time to understand the logic, which is pretty straigh forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_tree_leaf(node, x):\n",
    "    \"\"\"Get tree leaf score.\n",
    "    \n",
    "    Each node contains childres that are composed of aditiona nodes.\n",
    "    Final nodes with no children are the leaves.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    node: dict.\n",
    "        Node XGB dictionary.\n",
    "    x: dict.\n",
    "        Dictionary containing feature names and feature values.\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    score: float.\n",
    "        Leaf score.\n",
    "    \"\"\"\n",
    "\n",
    "    if 'leaf' in node:\n",
    "        # If the key leaf is found, the stop recurrency.\n",
    "        score = node['leaf']\n",
    "        return score\n",
    "    else:\n",
    "        # Get current split feature value\n",
    "        x_f_val = x[node['split']]\n",
    "\n",
    "        # Get next node.\n",
    "        if str(x_f_val) == 'nan':\n",
    "            # if split feature value is nan.\n",
    "            next_node_id = node['missing']\n",
    "        elif x_f_val < node['split_condition']:\n",
    "            # Split condition is true.\n",
    "            next_node_id = node['yes']\n",
    "        else:\n",
    "            # Split condition is false.\n",
    "            next_node_id = node['no']\n",
    "\n",
    "        # Dig down to the next node.\n",
    "        for children in node['children']:\n",
    "            if children['nodeid'] == next_node_id:\n",
    "                return get_tree_leaf(children, x)\n",
    "\n",
    "\n",
    "def binary_predict_proba(x, model_json):\n",
    "    \"\"\"Get score of a binary xgboost classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: dict.\n",
    "        Dictionary containing feature names and feature values.\n",
    "    \n",
    "    model_json: dict.\n",
    "        Dump of xgboost trees as json.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_score: list\n",
    "        Scores of the negative and positve class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get tree leafs.\n",
    "    tree_leaf_scores = []\n",
    "    for tree in model_json:\n",
    "        leaf_score = get_tree_leaf(\n",
    "            node=tree,\n",
    "            x=x)\n",
    "        tree_leaf_scores.append(leaf_score)\n",
    "\n",
    "    # Get logits.\n",
    "    logit = sum(tree_leaf_scores)\n",
    "    \n",
    "    # Compute logistic function\n",
    "    pos_class_probability = 1 / (1 + math.exp(-logit))\n",
    "\n",
    "    # Get negative and positive class probabilities.\n",
    "    y_score = [1 - pos_class_probability, pos_class_probability]\n",
    "    \n",
    "    return y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, this simple lines of code show the scores obtained using pure python code and the model json dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.303801\n",
       "1    0.694275\n",
       "2    0.694275\n",
       "3    0.694275\n",
       "4    0.694275\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Model object\n",
    "with open(MODEL_FILE_PATH, 'r') as f:\n",
    "    model_json = json.loads(f.read())\n",
    "\n",
    "y_scores_json = pd.Series([\n",
    "    binary_predict_proba(x.to_dict(), model_json)[1]\n",
    "    for _, x in X_test.iterrows()])\n",
    "\n",
    "y_scores_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores obtained manully and comparable to the scores obtained from the `xgboost.XGBClassifier` model, as shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.303801\n",
       "1    0.694275\n",
       "2    0.694275\n",
       "3    0.694275\n",
       "4    0.694275\n",
       "dtype: float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_scores_model = pd.Series(model.predict_proba(\n",
    "    X_test\n",
    ")[:, 1])\n",
    "\n",
    "y_scores_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the correctness of the score in the prescense of `NaN` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "0          NaN           NaN             NaN        NaN              NaN   \n",
       "\n",
       "   mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "0               NaN             NaN                  NaN            NaN   \n",
       "\n",
       "   mean_fractal_dimension  ...  worst_radius  worst_texture  worst_perimeter  \\\n",
       "0                     NaN  ...           NaN            NaN              NaN   \n",
       "\n",
       "   worst_area  worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "0         NaN               NaN                NaN              NaN   \n",
       "\n",
       "   worst_concave_points  worst_symmetry  worst_fractal_dimension  \n",
       "0                   NaN             NaN                      NaN  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nan = pd.DataFrame(\n",
    "    [[np.nan for _ in X.columns]],\n",
    "    columns=X.columns)\n",
    "x_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6942748733787976"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_predict_proba(x_nan.iloc[0].to_dict(), model_json)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69427484"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(x_nan)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this section we have show how to create a XGBoost model, saved it as a JSON set of trees and how to get model probabilities scores using pure python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Use Chalice to deploy your model.\n",
    "\n",
    "In this section, we will use Chalice in order to deploy our serverless infrastructure in AWS. \n",
    "\n",
    "(NOTE: Explain what is Chalice)\n",
    "\n",
    "You will first need to set up your AWS credentials as shown in the following [link](https://chalice.readthedocs.io/en/stable/quickstart.html#credentials), if it is your first time using AWS, don't worry, you'll basically have to create an account and get some credentials in order for you being able to take control of the cloud solutions provided by AWS programatillay.\n",
    "\n",
    "Once your AWS account and your credentials are all in place, we will only need to:\n",
    "\n",
    "- Create a new python environment and install the `chalice` package.\n",
    "- Clone the repository.\n",
    "- Local test and deploy to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new python environment and install the `chalice` package.\n",
    "\n",
    "First, create a python environment. You can use your favorite environment management tool, in this example I'll use conda:\n",
    "```\n",
    "$ conda create --name chalice_xgboost python=3.7.3\n",
    "$ conda activate chalice_xgboost\n",
    "(chalice_xgboost) $\n",
    "```\n",
    "\n",
    "Then install chalice package:\n",
    "```\n",
    "(chalice_xgboost) $ pip install chalice\n",
    "```\n",
    "\n",
    "## Clone the repository.\n",
    "\n",
    "In this section we'll clone the chalice project that contains our XGboost model dump, and place the terminal under the folder `chalice_xgboost`:\n",
    "\n",
    "``` \n",
    "(chalice_xgboost) $ git clone git@github.com:RaulSanchezVazquez/chalice_xgboost.git\n",
    "(chalice_xgboost) $ cd chalice_xgboost\n",
    "```\n",
    "\n",
    "(NOTE: Explain the folders)\n",
    "```\n",
    "(chalice_xgboost) chalice_xgboost $ ls\n",
    "README.ipynb README.md    app.py       chalicelib   models\n",
    "```\n",
    "\n",
    "## Local test and deploy to production.\n",
    "\n",
    "Run locally the server:\n",
    "\n",
    "```\n",
    "(chalice_xgboost) chalice_xgboost $ chalice local\n",
    "Serving on http://127.0.0.1:8000\n",
    "```\n",
    "\n",
    "The `app.py` file \n",
    "\n",
    "\n",
    "```python\n",
    "# Get model score.\n",
    "@app.route('/predict_proba', methods=['POST'])\n",
    "def get_decision():\n",
    "    \"\"\"Get model score.\n",
    "    \"\"\"\n",
    "    # Get body\n",
    "    body = app.current_request.json_body\n",
    "\n",
    "    # Get features from the request\n",
    "    x = body['x']\n",
    "    log.LOGGER_.info('x: %s' % x)\n",
    "\n",
    "    # Load the model.\n",
    "    model_json = model.get()\n",
    "    log.LOGGER_.info('Model loaded: %s Trees' % len(model_json))\n",
    "\n",
    "    # Get decision thresholds.\n",
    "    y_score = xgb.binary_predict_proba(x, model_json)\n",
    "    log.LOGGER_.info('y_score: %s' % y_score)\n",
    "\n",
    "    # Get response\n",
    "    response = {'y_score': y_score}\n",
    "    log.LOGGER_.info('response: %s' % response)\n",
    "\n",
    "    return {'response': response}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the server running locally, send a POST http request, and fetch the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_radius': 13.4,\n",
       " 'mean_texture': 20.52,\n",
       " 'mean_perimeter': 88.64,\n",
       " 'mean_area': 556.7,\n",
       " 'mean_smoothness': 0.1106,\n",
       " 'mean_compactness': 0.1469,\n",
       " 'mean_concavity': 0.1445,\n",
       " 'mean_concave_points': 0.08172,\n",
       " 'mean_symmetry': 0.2116,\n",
       " 'mean_fractal_dimension': 0.07325,\n",
       " 'radius_error': 0.3906,\n",
       " 'texture_error': 0.9306,\n",
       " 'perimeter_error': 3.093,\n",
       " 'area_error': 33.67,\n",
       " 'smoothness_error': nan,\n",
       " 'compactness_error': 0.02265,\n",
       " 'concavity_error': 0.03452,\n",
       " 'concave_points_error': 0.01334,\n",
       " 'symmetry_error': 0.01705,\n",
       " 'fractal_dimension_error': 0.004005,\n",
       " 'worst_radius': 16.41,\n",
       " 'worst_texture': 29.66,\n",
       " 'worst_perimeter': 113.3,\n",
       " 'worst_area': 844.4,\n",
       " 'worst_smoothness': 0.1574,\n",
       " 'worst_compactness': 0.3856,\n",
       " 'worst_concavity': 0.5106,\n",
       " 'worst_concave_points': 0.2051,\n",
       " 'worst_symmetry': 0.3585,\n",
       " 'worst_fractal_dimension': 0.1109}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_test.iloc[0].to_dict()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': {'y_score': [0.6961988994898333, 0.30380110051016673]}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "ENDPOINT = 'http://127.0.0.1:8000'\n",
    "\n",
    "body = {\"x\": x}\n",
    "\n",
    "response = http.request(\n",
    "     'POST',\n",
    "     ENDPOINT + '/predict_proba',\n",
    "     body=json.dumps(body).encode('utf-8'),\n",
    "     headers={'Content-Type': 'application/json'})\n",
    "\n",
    "response = json.loads(\n",
    "    response.data.decode('utf-8'))\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
